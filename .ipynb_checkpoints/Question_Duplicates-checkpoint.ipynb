{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac41b6a-1943-4059-b0f0-6c76289b168f",
   "metadata": {},
   "source": [
    "# 3:  Question duplicates\n",
    "\n",
    "In this, we will explore Siamese networks applied to natural language processing. we will further explore the fundamentals of TensorFlow and we will be able to implement a more complicated structure using it. We will learn how to implement models with different architectures. \n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Overview](#0)\n",
    "- [Part 1: Importing the Data](#1)\n",
    "    - [1.1 Loading in the data](#1.1)\n",
    "    - [1.2 Learn question encoding](#1.2)\n",
    "- [Part 2: Defining the Siamese model](#2)\n",
    "    - [2.1 Understanding the Siamese Network](#2.1)\n",
    "    - [2.2 Hard  Negative Mining](#2.2)\n",
    "- [Part 3: Training](#3)\n",
    "    - [3.1 Training the model](#3.1)\n",
    "- [Part 4: Evaluation](#4)\n",
    "    - [4.1 Evaluating our siamese network](#4.1)\n",
    "    - [4.2 Classify](#4.2)\n",
    "- [Part 5: Testing with our own questions](#5)\n",
    "- [On Siamese networks](#6)\n",
    "\n",
    "<a name='0'></a>\n",
    "### Overview\n",
    "In particular, in this we will: \n",
    "\n",
    "- Learn about Siamese networks\n",
    "- Understand how the triplet loss works\n",
    "- Understand how to evaluate accuracy\n",
    "- Use cosine similarity between the model's outputted vectors\n",
    "- Use the data generator to get batches of questions\n",
    "- Predict using our own model\n",
    "\n",
    "\n",
    "we will build a classifier that will allow to identify whether two questions are the same or not. \n",
    "\n",
    "<img src = \"./img/meme.png\" style=\"width:550px;height:300px;\"/>\n",
    "\n",
    "\n",
    "our model will take in the two questions, which will be transformed into tensors, each tensor will then go through embeddings, and after that an LSTM. Finally we will compare the outputs of the two subnetworks using cosine similarity. \n",
    "\n",
    "Before taking a deep dive into the model, we will start by importing the data set, and exploring it a bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c9e00-4293-458f-a5dc-604504d2fea6",
   "metadata": {},
   "source": [
    "###### <a name='1'></a>\n",
    "# Part 1: Importing the Data\n",
    "<a name='1.1'></a>\n",
    "### 1.1 Loading in the data\n",
    "\n",
    "we will be using the 'Quora question answer' dataset to build a model that can identify similar questions. This is a useful task because we don't want to have several versions of the same question posted. Several times when teaching I end up responding to similar questions on piazza, or on other community forums. This data set has already been labeled. Run the cell below to import some of the packages we will be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32b86b1-0131-4e64-a454-50551a090b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Set random seeds\n",
    "rnd.seed(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f6fd9-2796-4ac4-9a60-9bfde415eae5",
   "metadata": {},
   "source": [
    "we will now load the data set. We have done some preprocessing for you. this is a slightly different training method than the one we have seen but then don't worry about it, I  will explain everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbc9364-a3f8-4adc-9ba8-16623f6fa01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question pairs:  404351\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"questions.csv\")\n",
    "N = len(data)\n",
    "print('Number of question pairs: ', N)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff1109e-4a55-4ef5-89d7-8299930ada8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 300000 Test set: 10240\n"
     ]
    }
   ],
   "source": [
    "#split the data into a training and test set. The test set will be used later to evaluate our model.\n",
    "N_train = 300000\n",
    "N_test = 10240\n",
    "data_train = data[:N_train]\n",
    "data_test = data[N_train:N_train + N_test]\n",
    "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n",
    "del (data)  # remove to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf684ae-9a40-49de-bfa9-5b63d12df98f",
   "metadata": {},
   "source": [
    "we will select only the question pairs that are duplicate to train the model. <br>\n",
    "we need to build two sets of questions as input for the Siamese network, assuming that question $q1_i$ (question $i$ in the first set) is a duplicate of $q2_i$ (question $i$ in the second set), but all other questions in the second set are not duplicates of $q1_i$.  \n",
    "The test set uses the original pairs of questions and the status describing if the questions are duplicates.\n",
    "\n",
    "The following cells are in charge of selecting only duplicate questions from the training set, which will give us a smaller dataset. First find the indexes with duplicate questions.\n",
    "\n",
    "we will start by identifying the indexes in the training set which correspond to duplicate questions. For this we will define a boolean variable `td_index`, which has value `True` if the index corresponds to duplicate questions and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4a67dd-e62a-4253-a6d6-4502a2b89ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "Indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
     ]
    }
   ],
   "source": [
    "td_index = data_train['is_duplicate'] == 1\n",
    "td_index = [i for i, x in enumerate(td_index) if x]\n",
    "print('Number of duplicate questions: ', len(td_index))\n",
    "print('Indexes of first ten duplicate questions:', td_index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25fe3d-4b40-4f95-bbe3-a166b5ba7e05",
   "metadata": {},
   "source": [
    "we will first need to split the data into a training and test set. The test set will be used later to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b16b036-8753-4cfc-b4b0-1e788e7d197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
      "is_duplicate:  1\n"
     ]
    }
   ],
   "source": [
    "print(data_train['question1'][5])\n",
    "print(data_train['question2'][5])\n",
    "print('is_duplicate: ', data_train['is_duplicate'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27719d-fedb-4892-9986-7112ed9fd0a4",
   "metadata": {},
   "source": [
    "Next, keep only the rows in the original training set that correspond to the rows where `td_index` is `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c73c28-9843-41a6-aed2-7aebea1083d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_train = np.array(data_train['question1'][td_index])\n",
    "Q2_train = np.array(data_train['question2'][td_index])\n",
    "\n",
    "Q1_test = np.array(data_test['question1'])\n",
    "Q2_test = np.array(data_test['question2'])\n",
    "y_test  = np.array(data_test['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32374d7d-ed41-48d6-9ff2-652f904d9a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING QUESTIONS:\n",
      "\n",
      "Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "Question 2:  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? \n",
      "\n",
      "Question 1:  What would a Trump presidency mean for current international masterâ€™s students on an F1 visa?\n",
      "Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? \n",
      "\n",
      "TESTING QUESTIONS:\n",
      "\n",
      "Question 1:  How do I prepare for interviews for cse?\n",
      "Question 2:  What is the best way to prepare for cse? \n",
      "\n",
      "is_duplicate = 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's print to see what our data looks like.\n",
    "print('TRAINING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_train[0])\n",
    "print('Question 2: ', Q2_train[0], '\\n')\n",
    "print('Question 1: ', Q1_train[5])\n",
    "print('Question 2: ', Q2_train[5], '\\n')\n",
    "\n",
    "print('TESTING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_test[0])\n",
    "print('Question 2: ', Q2_test[0], '\\n')\n",
    "print('is_duplicate =', y_test[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba6b434-bcf3-403b-aa61-41957e573a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "The length of the training set is:   89188\n",
      "The length of the validation set is:  22298\n"
     ]
    }
   ],
   "source": [
    "#Finally, split our training set into training/validation sets so that we can use them at training time.\n",
    "cut_off = int(len(Q1_train) * 0.8)\n",
    "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
    "val_Q1, val_Q2 = Q1_train[cut_off:], Q2_train[cut_off:]\n",
    "print('Number of duplicate questions: ', len(Q1_train))\n",
    "print(\"The length of the training set is:  \", len(train_Q1))\n",
    "print(\"The length of the validation set is: \", len(val_Q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f09dc-8aae-467b-a8d5-e9c80d48ff31",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 Learning question encoding\n",
    "\n",
    "The next step is to learn how to encode each of the questions as a list of numbers (integers). we will be learning how to encode each word of the selected duplicate pairs with an index. \n",
    "\n",
    "we will start by learning a word dictionary, or vocabulary, containing all the words in our training dataset, which we will use to encode each word of the selected duplicate pairs with an index. \n",
    "\n",
    "For this task we will be using the [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer from Keras. which will take care of everything for you. Begin by setting a seed, so we all get the same encoding.\n",
    "\n",
    "The vocabulary is learned using the `.adapt()`. This will analyze the dataset, determine the frequency of individual string values, and create a vocabulary from them. If we need, we can later access the vocabulary by using `.get_vocabulary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "228429b6-ad51-47ce-845f-a992462f9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "text_vectorization = tf.keras.layers.TextVectorization(output_mode='int',split='whitespace', standardize='strip_punctuation')\n",
    "text_vectorization.adapt(np.concatenate((Q1_train,Q2_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fdbcb1-5802-41d3-aba8-7ff2fd364769",
   "metadata": {},
   "source": [
    "As we can see, it is set to split text on whitespaces and it's stripping the punctuation from text. we can check how big our vocabulary is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0af31cc-35d4-4f11-8bce-d8e5ee66ba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36222\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary size: {text_vectorization.vocabulary_size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2155f26-1636-4c22-a8a1-84fb583e7cb4",
   "metadata": {},
   "source": [
    "we can also call `text_vectorization` to see what the encoding looks like for the first questions of the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ba0bc2-1770-47b6-a05b-1651fb2e37fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first question in the train set:\n",
      "\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
      "\n",
      "encoded version:\n",
      "tf.Tensor(\n",
      "[ 6984     6   178    10  8988  2442 35391   761    13  6636 28204    31\n",
      "    28   483    45    98], shape=(16,), dtype=int64) \n",
      "\n",
      "first question in the test set:\n",
      "\n",
      "How do I prepare for interviews for cse? \n",
      "\n",
      "encoded version:\n",
      "tf.Tensor([    4     8     6   160    17  2079    17 11775], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print('first question in the train set:\\n')\n",
    "print(Q1_train[0], '\\n') \n",
    "print('encoded version:')\n",
    "print(text_vectorization(Q1_train[0]),'\\n')\n",
    "\n",
    "print('first question in the test set:\\n')\n",
    "print(Q1_test[0], '\\n')\n",
    "print('encoded version:')\n",
    "print(text_vectorization(Q1_test[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c8922-6740-4adb-8c8f-13cd26f368ea",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# Part 2: Defining the Siamese model\n",
    "\n",
    "<a name='2.1'></a>\n",
    "\n",
    "### 2.1 Understanding the Siamese Network \n",
    "A Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. The Siamese network we are about to implement looks something like this:\n",
    "\n",
    "<img src = \"./img/Siamese.png\" style=\"width:790px;height:300px;\"/>\n",
    "\n",
    "we get the question, get it vectorized and embedded, run it through an LSTM layer, normalize $v_1$ and $v_2$, and finally get the corresponding cosine similarity for each pair of questions (remember that each question is a single string). Because of the implementation of the loss function we will see in the next section, we are not going to have the cosine similarity as output of our Siamese network, but rather $v_1$ and $v_2$. we will add the cosine distance step once we reach the classification step. \n",
    "\n",
    "To train the model, we will use the triplet loss (explained below). This loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The (cosine) distance from the baseline input to the positive input is minimized, and the distance from the baseline input to the negative  input is maximized. Mathematically, we are trying to maximize the following.\n",
    "\n",
    "$$\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right),$$\n",
    "\n",
    "where $A$ is the anchor input, for example $q1_1$, $P$ is the duplicate input, for example, $q2_1$, and $N$ is the negative input (the non duplicate question), for example $q2_2$.<br>\n",
    "$\\alpha$ is a margin; we can think about it as a safety net, or by how much we want to push the duplicates from the non duplicates. This is the essence of the triplet loss. However, as we will see in the next section, we will be using a pretty smart trick to improve our training, known as hard negative mining. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a32422-b457-47b9-beaf-e7df60d34205",
   "metadata": {},
   "source": [
    "<a name='ex02'></a>\n",
    "### Siamese\n",
    "\n",
    "**Instructions:** Let's Implement the `Siamese` function below. we should be using all the functions explained below. \n",
    "\n",
    "To implement this model, we will be using `TensorFlow`. Concretely, we will be using the following functions.\n",
    "\n",
    "\n",
    "- [`tf.keras.models.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential): groups a linear stack of layers into a tf.keras.Model.\n",
    "    - we can pass in the layers as arguments to `Serial`, separated by commas, or simply instantiate the `Sequential`model and use the `add` method to add layers.\n",
    "    - For example: `Sequential(Embeddings(...), AveragePooling1D(...), Dense(...), Softmax(...))` or \n",
    "    \n",
    "    `model = Sequential()\n",
    "     model.add(Embeddings(...))\n",
    "     model.add(AveragePooling1D(...))\n",
    "     model.add(Dense(...))\n",
    "     model.add(Softmax(...))`\n",
    "\n",
    "-  [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) : Maps positive integers into vectors of fixed size. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (called `d_feature`in the model) is the number of elements in the word embedding. \n",
    "    - `Embedding(input_dim, output_dim)`.\n",
    "    - `input_dim` is the number of unique words in the given vocabulary.\n",
    "    - `output_dim` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
    "    \n",
    "\n",
    "\n",
    "-  [`tf.keras.layers.LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) : The LSTM layer. The number of units should be specified and should match the number of elements in the word embedding. \n",
    "    - `LSTM(units)` Builds an LSTM layer of n_units.\n",
    "    \n",
    "    \n",
    "    \n",
    "- [`tf.keras.layers.GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) : Computes global average pooling, which essentially takes the mean across a desired axis. GlobalAveragePooling1D uses one tensor axis to form groups of values and replaces each group with the mean value of that group. \n",
    "    - `GlobalAveragePooling1D()` takes the mean.\n",
    "\n",
    "\n",
    "\n",
    "- [`tf.keras.layers.Lambda`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn): Layer with no weights that applies the function f, which should be specified using a lambda syntax. we will use this layer to apply normalization with the function\n",
    "    - `tfmath.l2_normalize(x)`\n",
    "\n",
    "\n",
    "\n",
    "- [`tf.keras.layers.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input): it is used to instantiate a Keras tensor. Remember to set correctly the dimension and type of the input, which are batches of questions. For this, keep in mind that each question is a single string. \n",
    "    - `Input(input_shape,dtype=None,...)`\n",
    "    - `input_shape`: Shape tuple (not including the batch axis)\n",
    "    - `dtype`: (optional) data type of the input\n",
    "\n",
    "\n",
    "\n",
    "- [`tf.keras.layers.Concatenate`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate): Layer that concatenates a list of inputs. This layer will concatenate the normalized outputs of each LSTM into a single output for the model. \n",
    "    - `Concatenate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a709d5a5-8278-4f3f-9f39-992b18393f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: Siamese\n",
    "def Siamese(text_vectorizer, vocab_size=36224, d_feature=128):\n",
    "    \"\"\"Returns a Siamese model.\n",
    "\n",
    "    Args:\n",
    "        text_vectorizer (TextVectorization): TextVectorization instance, already adapted to our training data.\n",
    "        vocab_size (int, optional): Length of the vocabulary. Defaults to 36224, which is the vocabulary size for our case.\n",
    "        d_model (int, optional): Depth of the model. Defaults to 128.\n",
    "        \n",
    "    Returns:\n",
    "        tf.model.Model: A Siamese model. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ###\n",
    "\n",
    "    branch = tf.keras.models.Sequential(name='sequential') \n",
    "    # Add the text_vectorizer layer. This is the text_vectorizer we instantiated and trained before \n",
    "    branch.add(text_vectorizer)\n",
    "    # Add the Embedding layer. Remember to call it 'embedding' using the parameter `name`\n",
    "    branch.add(tf.keras.layers.Embedding(vocab_size, d_feature, name=\"embedding\"))\n",
    "    # Add the LSTM layer, recall from W2 that we want to the LSTM layer to return sequences, ot just one value. \n",
    "    # Remember to call it 'LSTM' using the parameter `name`\n",
    "    branch.add(tf.keras.layers.LSTM(d_feature,return_sequences=True, name=\"LSTM\"))\n",
    "    # Add the GlobalAveragePooling1D layer. Remember to call it 'mean' using the parameter `name`\n",
    "    branch.add(tf.keras.layers.GlobalAveragePooling1D(name=\"mean\"))\n",
    "    # Add the normalizing layer using the Lambda function. Remember to call it 'out' using the parameter `name`\n",
    "    branch.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x), name=\"out\"))\n",
    "    \n",
    "    # Define both inputs. Remember to call then 'input_1' and 'input_2' using the `name` parameter. \n",
    "    # Be mindful of the data type and size\n",
    "    input1 = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='input_1')\n",
    "    input2 = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='input_2')\n",
    "    # Define the output of each branch of our Siamese network. Remember that both branches have the same coefficients, \n",
    "    # but they each receive different inputs.\n",
    "    branch1 = branch(input1)\n",
    "    branch2 = branch(input2)\n",
    "    # Define the Concatenate layer. we should concatenate columns, we can fix this using the `axis`parameter. \n",
    "    # This layer is applied over the outputs of each branch of the Siamese network\n",
    "    conc = tf.keras.layers.Concatenate(axis=1, name='conc_1_2')([branch1, branch2]) \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=[input1, input2], outputs=conc, name=\"SiameseModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c5b69a-b6a1-40f1-b196-81e276a339f4",
   "metadata": {},
   "source": [
    "Setup the Siamese network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c97797c4-2d6c-402b-8974-1da0e033185b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseModel\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 128)          4768000     ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conc_1_2 (Concatenate)         (None, 256)          0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,768,000\n",
      "Trainable params: 4,768,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         4636416   \n",
      "                                                                 \n",
      " LSTM (LSTM)                 (None, None, 128)         131584    \n",
      "                                                                 \n",
      " mean (GlobalAveragePooling1  (None, 128)              0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " out (Lambda)                (None, 128)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,768,000\n",
      "Trainable params: 4,768,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check our model\n",
    "model = Siamese(text_vectorization, vocab_size=text_vectorization.vocabulary_size())\n",
    "model.build(input_shape=None)\n",
    "model.summary()\n",
    "model.get_layer(name='sequential').summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2bec5-1e71-400f-a3d0-d9a10773b1fc",
   "metadata": {},
   "source": [
    "##### we can also draw the model for a clearer view of our Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4bf92bb-55b6-4653-ac21-7a87d5c3fed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e87462-b5f3-4b27-b946-52864fc47213",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "\n",
    "### 2.2 Hard Negative Mining\n",
    "\n",
    "\n",
    "We will now implement the `TripletLoss` with hard negative mining.<br>\n",
    "We will be using all the questions from each batch to compute this loss. Positive examples are questions $q1_i$, and $q2_i$, while all the other combinations $q1_i$, $q2_j$ ($i\\neq j$), are considered negative examples. The loss will be composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the *closest negative*. Our loss expression is then:\n",
    " \n",
    "\\begin{align}\n",
    " \\mathcal{Loss_1(A,P,N)} &=\\max \\left( -cos(A,P)  + mean_{neg} +\\alpha, 0\\right) \\\\\n",
    " \\mathcal{Loss_2(A,P,N)} &=\\max \\left( -cos(A,P)  + closest_{neg} +\\alpha, 0\\right) \\\\\n",
    "\\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Further, two sets of instructions are provided. The first set, found just below, provides a brief description of the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b5cd48-26fa-4272-8d3e-4ab5aac7e09c",
   "metadata": {},
   "source": [
    "<a name='ex03'></a>\n",
    "### TripletLossFn\n",
    "\n",
    "**Instructions (Brief):** Here is a list of things we should do: <br>\n",
    "\n",
    "- As this will be run inside Tensorflow, use all operation supplied by `tf.math` or `tf.linalg`, instead of `numpy` functions. we will also need to explicitly use `tf.shape` to get the batch size from the inputs. This is to make it compatible with the Tensor inputs it will receive when doing actual training and testing. \n",
    "- Use [`tf.linalg.matmul`](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) to calculate the similarity matrix $v_2v_1^T$ of dimension `batch_size` x `batch_size`. \n",
    "- Take the score of the duplicates on the diagonal with [`tf.linalg.diag_part`](https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part). \n",
    "- Use the `TensorFlow` functions [`tf.eye`](https://www.tensorflow.org/api_docs/python/tf/eye) and [`tf.math.reduce_max`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_max) for the identity matrix and the maximum respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f71bb66-1ee0-464c-9f05-0e22da47f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: TripletLossFn\n",
    "def TripletLossFn(v1, v2,  margin=0.25):\n",
    "    \"\"\"Custom Loss function.\n",
    "\n",
    "    Args:\n",
    "        v1 (numpy.ndarray or Tensor): Array with dimension (batch_size, model_dimension) associated to Q1.\n",
    "        v2 (numpy.ndarray or Tensor): Array with dimension (batch_size, model_dimension) associated to Q2.\n",
    "        margin (float, optional): Desired margin. Defaults to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss (numpy.ndarray or Tensor)\n",
    "    \"\"\"\n",
    "   \n",
    "    ###\n",
    "\n",
    "    # use `tf.linalg.matmul` to take the dot product of the two batches. \n",
    "    # Don't forget to transpose the second argument using `transpose_b=True`\n",
    "    scores = tf.linalg.matmul(v2, v1, transpose_b=True)\n",
    "    # calculate new batch size and cast it as the same datatype as scores. \n",
    "\n",
    "    batch_size = tf.cast(tf.shape(v1)[0], scores.dtype) \n",
    "    # use `tf.linalg.diag_part` to grab the cosine similarity of all positive examples\n",
    "    positive = tf.linalg.diag_part(scores)\n",
    "    # subtract the diagonal from scores. we can do this by creating a diagonal matrix with the values \n",
    "    # of all positive examples using `tf.linalg.diag`\n",
    "    negative_zero_on_duplicate = scores - tf.linalg.diag(positive)\n",
    "    # use `tf.math.reduce_sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)`\n",
    "    mean_negative = tf.math.reduce_sum(negative_zero_on_duplicate, axis=1) / (batch_size - 1)\n",
    "    # create a composition of two masks: \n",
    "    # the first mask to extract the diagonal elements (make sure we use the variable batch_size here), \n",
    "    # the second mask to extract elements in the negative_zero_on_duplicate matrix that are larger than the elements in the diagonal \n",
    "    mask_exclude_positives = tf.cast((tf.eye(batch_size) == 1)|(negative_zero_on_duplicate > tf.expand_dims(positive, 1)),\n",
    "                                    scores.dtype)\n",
    "    # multiply `mask_exclude_positives` with 2.0 and subtract it out of `negative_zero_on_duplicate`\n",
    "    negative_without_positive = negative_zero_on_duplicate - (mask_exclude_positives * 2)\n",
    "    # take the row by row `max` of `negative_without_positive`. \n",
    "    # Hint: `tf.math.reduce_max(negative_without_positive, axis = None)`\n",
    "    closest_negative = tf.math.reduce_max(negative_without_positive, axis=1)\n",
    "    # compute `tf.maximum` among 0.0 and `A`\n",
    "    # A = subtract `positive` from `margin` and add `closest_negative` \n",
    "    triplet_loss1 = tf.maximum(0.0, margin - positive + closest_negative)\n",
    "    # compute `tf.maximum` among 0.0 and `B`\n",
    "    # B = subtract `positive` from `margin` and add `mean_negative` \n",
    "    triplet_loss2 = tf.maximum(0.0, margin  - positive + mean_negative)\n",
    "    # add the two losses together and take the `tf.math.reduce_sum` of it\n",
    "    triplet_loss = tf.math.reduce_sum(triplet_loss1 + triplet_loss2)\n",
    "    \n",
    "    ###\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7022e2d-b4c3-48c0-b720-47a7cd9d0773",
   "metadata": {},
   "source": [
    "Now we can check the triplet loss between two sets. The following example emulates the triplet loss between two groups of questions with `batch_size=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43c0ca3e-4a0f-450d-895d-80061b3e68b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: 0.7035076825158911\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
    "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
    "print(\"Triplet Loss:\", TripletLossFn(v1,v2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f119ef-c061-4f5e-ab4e-fcb6e93bdad6",
   "metadata": {},
   "source": [
    "To recognize it as a loss function, keras needs it to have two inputs: true labels, and output labels. we will not be using the true labels, but we still need to pass some dummy variable with size `(batch_size,)` for TensorFlow to accept it as a valid loss.\n",
    "\n",
    "Additionally, the `out` parameter must coincide with the output of our Siamese network, which is the concatenation of the processing of each of the inputs, so we need to extract $v_1$ and $v_2$ from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff27da0-fa5c-4232-8b57-ab64abbfd1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLoss(labels, out, margin=0.25):\n",
    "    _, out_size = out.shape # get embedding size\n",
    "    v1 = out[:,:int(out_size/2)] # Extract v1 from out\n",
    "    v2 = out[:,int(out_size/2):] # Extract v2 from out\n",
    "    return TripletLossFn(v1, v2, margin=margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550e9d2-0e75-4a62-a2e4-490c268a9a34",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "\n",
    "# Part 3: Training\n",
    "\n",
    "Now it's time to finally train our model. As usual, we have to define the cost function and the optimizer. we also have to build the actual model we will be training. \n",
    "\n",
    "To pass the input questions for training and validation we will use the iterator produced by [`tensorflow.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Run the next cell to create our train and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d98fd674-1e89-4d9e-9ee4-e60d63ade3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(((train_Q1, train_Q2),tf.constant([1]*len(train_Q1))))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((val_Q1, val_Q2),tf.constant([1]*len(val_Q1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1be3dd-a49f-4f93-99cb-2d8f71a9edcd",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "\n",
    "### 3.1 Training the model\n",
    "\n",
    "we will now write a function that takes in our model to train it. To train our model we have to decide how many times we want to iterate over the entire data set; each iteration is defined as an `epoch`. For each epoch, we have to go over all the data, using our `Dataset` iterator.\n",
    "\n",
    "<a name='ex04'></a>\n",
    "### train_model\n",
    "\n",
    "Now lets implement the `train_model` below to train the neural network above. Here is a list of things we should do: \n",
    "\n",
    "- Compile the model. Here we will need to pass in:\n",
    "    - `loss=TripletLoss`\n",
    "    - `optimizer=Adam()` with learning rate `lr`\n",
    "- Call the `fit` method. we should pass:\n",
    "    - `train_dataset`\n",
    "    - `epochs`\n",
    "    - `validation_data` \n",
    "\n",
    "we will be using our triplet loss function with Adam optimizer. Also, note that we are not explicitly defining the batch size, because it will be already determined by the `Dataset`.\n",
    "\n",
    "This function will return the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b26a133-0a5c-43fb-9ef8-0f7dabc0a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: train_model\n",
    "def train_model(Siamese, TripletLoss, text_vectorizer, train_dataset, val_dataset, d_feature=128, lr=0.01, train_steps=5):\n",
    "    \"\"\"Training the Siamese Model\n",
    "\n",
    "    Args:\n",
    "        Siamese (function): Function that returns the Siamese model.\n",
    "        TripletLoss (function): Function that defines the TripletLoss loss function.\n",
    "        text_vectorizer: trained instance of `TextVecotrization` \n",
    "        train_dataset (tf.data.Dataset): Training dataset\n",
    "        val_dataset (tf.data.Dataset): Validation dataset\n",
    "        d_feature (int, optional) = size of the encoding. Defaults to 128.\n",
    "        lr (float, optional): learning rate for optimizer. Defaults to 0.01\n",
    "        train_steps (int): number of epochs\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model\n",
    "    \"\"\"\n",
    "    ###\n",
    "\n",
    "    # Instantiate our Siamese model\n",
    "    model = Siamese(text_vectorizer,\n",
    "                    vocab_size = text_vectorizer.vocabulary_size(), #set vocab_size accordingly to the size of our vocabulary\n",
    "                    d_feature = d_feature)\n",
    "    # Compile the model\n",
    "    model.compile(loss=TripletLoss,\n",
    "                  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            )\n",
    "    # Train the model \n",
    "    model.fit(train_dataset,\n",
    "              epochs = train_steps,\n",
    "              validation_data = val_dataset,\n",
    "             )\n",
    "             \n",
    "    ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5914e2-1e60-442d-b793-1917ec49b4af",
   "metadata": {},
   "source": [
    "Now call the `train_model` function. we will be using a batch size of 256. \n",
    "\n",
    "To create the data generators we will be using the method `batch` for `Dataset` object. we will also call the `shuffle` method, to shuffle the dataset on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b3c894e-bc5b-40bc-91e8-4fa30e0400c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "697/697 [==============================] - 37s 47ms/step - loss: 62.9585 - val_loss: 62.5773\n",
      "Epoch 2/2\n",
      "697/697 [==============================] - 32s 46ms/step - loss: 62.5649 - val_loss: 62.4910\n"
     ]
    }
   ],
   "source": [
    "train_steps = 2\n",
    "batch_size = 128\n",
    "train_generator = train_dataset.shuffle(len(train_Q1),\n",
    "                                        seed=7, \n",
    "                                        reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "val_generator = val_dataset.shuffle(len(val_Q1), \n",
    "                                   seed=7,\n",
    "                                   reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "model = train_model(Siamese, TripletLoss,text_vectorization, \n",
    "                                            train_generator, \n",
    "                                            val_generator, \n",
    "                                            train_steps=train_steps,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b473ef82-e9b5-48c3-88e9-18d099c50172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/my_mod\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/my_mod\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model/my_mod', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d839e-3143-4328-a168-c52bfb756000",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "\n",
    "# Part 4:  Evaluation  \n",
    "\n",
    "<a name='4.1'></a>\n",
    "\n",
    "### 4.1 Evaluating our siamese network\n",
    "\n",
    "In this section we will learn how to evaluate a Siamese network. we will start by loading a pretrained model, and then we will use it to predict. For the prediction we will need to take the output of our model and compute the cosine loss between each pair of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08bee5a1-081a-41b5-b0d3-d5ea9938d45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseModel\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 128)          4768000     ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conc_1_2 (Concatenate)         (None, 256)          0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,768,000\n",
      "Trainable params: 4,768,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# WORKS WITH KERAS VERSION < 3\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model with the custom loss function\n",
    "model = load_model('model/my_mod', custom_objects={'TripletLoss': TripletLoss})\n",
    "\n",
    "# Show the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6f8a09b-fbb6-4bdb-9895-81674d50705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT - Works with Keras Version 3 ( AND IS A BETTER TRAINED MODEL WITH ACCURACY OF > 75% )\n",
    "# model = tf.keras.models.load_model('model/trained_model.keras', safe_mode=False, compile=False)\n",
    "# Show the model architecture\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e0e62-bd43-4971-b886-3c8fe9bd9c01",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "### 4.2 Classify\n",
    "To determine the accuracy of the model, we will use the test set that was configured earlier. While in training we used only positive examples, the test data, `Q1_test`, `Q2_test` and `y_test`, is set up as pairs of questions, some of which are duplicates and some are not. \n",
    "This routine will run all the test question pairs through the model, compute the cosine similarity of each pair, threshold it and compare the result to `y_test` - the correct response from the data set. The results are accumulated to produce an accuracy; the confusion matrix is also computed to have a better understanding of the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248ea47-dab3-45a1-9363-2e85f94353ba",
   "metadata": {},
   "source": [
    "<a name='ex05'></a>\n",
    "### classify\n",
    "\n",
    "**Instructions**  \n",
    " - Use a `tensorflow.data.Dataset` to go through the data in chunks with size batch_size. This time we don't need the labels, so we can just replace them by `None`,\n",
    " - use `predict` on the chunks of data.\n",
    " - compute `v1`, `v2` using the model output,\n",
    " - for each element of the batch\n",
    "        - compute the cosine similarity of each pair of entries, `v1[j]`,`v2[j]`\n",
    "        - determine if `d > threshold`\n",
    "        - increment accuracy if that result matches the expected results (`y_test[j]`)\n",
    "  \n",
    "   Instead of running a for loop, we will vectorize all these operations to make things more efficient,\n",
    " - compute the final accuracy and confusion matrix and return. For the confusion matrix we can use the [`tf.math.confusion_matrix`](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90ca2113-40fa-4a75-a658-138a21702800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: classify\n",
    "def classify(test_Q1, test_Q2, y_test, threshold, model, batch_size=64, verbose=True):\n",
    "    \"\"\"Function to test the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        test_Q1 (numpy.ndarray): Array of Q1 questions. Each element of the array would be a string.\n",
    "        test_Q2 (numpy.ndarray): Array of Q2 questions. Each element of the array would be a string.\n",
    "        y_test (numpy.ndarray): Array of actual target.\n",
    "        threshold (float): Desired threshold\n",
    "        model (tensorflow.Keras.Model): The Siamese model.\n",
    "        batch_size (int, optional): Size of the batches. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model\n",
    "        numpy.array: confusion matrix\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    test_gen = tf.data.Dataset.from_tensor_slices(((test_Q1, test_Q2),None)).batch(batch_size=batch_size)\n",
    "    \n",
    "    ###\n",
    "\n",
    "    pred = model.predict(test_gen)\n",
    "    _, n_feat = pred.shape\n",
    "    v1 = pred[:, :int(n_feat/2)]\n",
    "    v2 = pred[:, int(n_feat/2):]\n",
    "    \n",
    "    # Compute the cosine similarity. Using `tf.math.reduce_sum`. \n",
    "    # Don't forget to use the appropriate axis argument.\n",
    "    d  = tf.reduce_sum(tf.multiply(v1, v2), axis=1) / (tf.norm(v1, axis=1) * tf.norm(v2, axis=1))\n",
    "    # Check if d>threshold to make predictions\n",
    "    y_pred = tf.cast(d > threshold, tf.float64)\n",
    "    # take the average of correct predictions to get the accuracy\n",
    "    correct_num = tf.equal(y_test, y_pred)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_num,tf.float64))\n",
    "    # compute the confusion matrix using `tf.math.confusion_matrix`\n",
    "    cm = tf.math.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    return accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78ee8ed1-f69c-476b-8d72-73a2f16cca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 52ms/step\n",
      "Accuracy 0.5998046875\n",
      "Confusion matrix:\n",
      "[[2931 3451]\n",
      " [ 647 3211]]\n"
     ]
    }
   ],
   "source": [
    "# this takes around 1 minute\n",
    "accuracy, cm = classify(Q1_test,Q2_test, y_test, 0.7, model,  batch_size = 512) \n",
    "print(\"Accuracy\", accuracy.numpy())\n",
    "print(f\"Confusion matrix:\\n{cm.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a3df7-5c7d-4912-b884-b9a13f961cf7",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "\n",
    "# Part 5: Testing with our own questions\n",
    "\n",
    "In this final section we will test the model with our own questions. we will write a function `predict` which takes two questions as input and returns `True` or `False` depending on whether the question pair is a duplicate or not.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a212756-8773-49aa-bdae-cd6cd8b1a2d1",
   "metadata": {},
   "source": [
    "Write a function `predict` that takes in two questions, the threshold and the model, and returns whether the questions are duplicates (`True`) or not duplicates (`False`) given a similarity threshold. \n",
    "\n",
    "<a name='ex06'></a>\n",
    "### Predict\n",
    "\n",
    "\n",
    "**Instructions:** \n",
    "- Create a tensorflow.data.Dataset from our two questions. Again, labels are not important, so we simply write `None`\n",
    "- use the trained model output to create `v1`, `v2`\n",
    "- compute the cosine similarity (dot product) of `v1`, `v2`\n",
    "- compute `res` by comparing d to the threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd16624d-9d4d-4e6d-ab3d-d821c0cebcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: predict\n",
    "def predict(question1, question2, threshold, model, verbose=False):\n",
    "    \"\"\"Function for predicting if two questions are duplicates.\n",
    "\n",
    "    Args:\n",
    "        question1 (str): First question.\n",
    "        question2 (str): Second question.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (tensorflow.keras.Model): The Siamese model.\n",
    "        verbose (bool, optional): If the results should be printed out. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the questions are duplicates, False otherwise.\n",
    "    \"\"\"\n",
    "    generator = tf.data.Dataset.from_tensor_slices((([question1], [question2]),None)).batch(batch_size=1)\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Call the predict method of our model and save the output into v1v2\n",
    "    v1v2 = model.predict(generator)\n",
    "    # Extract v1 and v2 from the model output\n",
    "    v1 = v1v2[:, :int(v1v2.shape[1]/2)]\n",
    "    v2 = v1v2[:, int(v1v2.shape[1]/2):]\n",
    "    # Take the dot product to compute cos similarity of each pair of entries, v1, v2\n",
    "    # Since v1 and v2 are both vectors, use the function tf.math.reduce_sum instead of tf.linalg.matmul\n",
    "    d = tf.math.reduce_sum(tf.multiply(v1, v2))\n",
    "    # Is d greater than the threshold?\n",
    "    res = d > threshold\n",
    "\n",
    "    ###\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"Q1  = \", question1, \"\\nQ2  = \", question2)\n",
    "        print(\"d   = \", d.numpy())\n",
    "        print(\"res = \", res.numpy())\n",
    "\n",
    "    return res.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dab0b5a3-f408-469a-a84c-b2ac0ad01e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "Q1  =  When will I see you? \n",
      "Q2  =  When can I see you again?\n",
      "d   =  0.8182352\n",
      "res =  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to try with our own questions\n",
    "question1 = \"When will I see you?\"\n",
    "question2 = \"When can I see you again?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cc0fa73-82ac-4d0f-a374-a848ed25cdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "Q1  =  Do they enjoy eating the dessert? \n",
      "Q2  =  Do they like hiking in the desert?\n",
      "d   =  0.09466209\n",
      "res =  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to try with our own questions\n",
    "question1 = \"Do they enjoy eating the dessert?\"\n",
    "question2 = \"Do they like hiking in the desert?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d433349-09ba-4a1e-a518-f6b5b0cf4633",
   "metadata": {},
   "source": [
    "we can see that the Siamese network is capable of catching complicated structures. Concretely it can identify question duplicates although the questions do not have many words in common. \n",
    "\n",
    "<a name='6'></a>\n",
    "\n",
    "### On Siamese networks\n",
    "\n",
    "Siamese networks are important and useful. Many times there are several questions that are already asked in quora, or other platforms and we can use Siamese networks to avoid question duplicates. \n",
    "\n",
    "Congratulations, we have now built a powerful system that can recognize question duplicates.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
